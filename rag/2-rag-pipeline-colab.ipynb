{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL_CYd-sHbcF"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "%pip uninstall -y langchain-core langchain-openai langchain-experimental beautifulsoup4 langchain-community langchain chromadb beautifulsoup4\n",
        "\n",
        "# Install compatible versions of langchain-core and langchain-openai\n",
        "%pip install langchain-core==0.3.6\n",
        "%pip install langchain-openai==0.2.1\n",
        "%pip install langchain-experimental==0.3.2\n",
        "%pip install langchain-community==0.3.1\n",
        "%pip install langchain==0.3.1\n",
        "\n",
        "# Install remaining packages\n",
        "%pip install chromadb==0.5.11\n",
        "%pip install beautifulsoup4==4.12.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jFnE-ViHaDv"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW7v8n5hHaD0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import openai\n",
        "import chromadb\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "from langchain_openai import ChatOpenAI #, OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from langchain import hub\n",
        "from google.colab import userdata\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK7Dw5SnHaD1"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY']= userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['USER_AGENT'] = 'RAGUserAgent'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4atybmtAJAgL"
      },
      "outputs": [],
      "source": [
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhdLHWC1HaD2"
      },
      "source": [
        "### Load documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZTPBUGZHaD4"
      },
      "outputs": [],
      "source": [
        "path = 'http://kbourne.github.io/chapter1.html'\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(path,),\n",
        "    bs_kwargs=dict\n",
        "            (\n",
        "                parse_only=bs4.SoupStrainer(\n",
        "                    class_=('post-content','post-title','post-header')\n",
        "                )\n",
        "            )\n",
        "    )\n",
        "\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFJyovtoHaD4"
      },
      "source": [
        "### Split documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQiJECfIHaD5"
      },
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(gemini_embeddings)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paQa1Fq8NWyS"
      },
      "outputs": [],
      "source": [
        "vector_store = Chroma.from_documents(documents=splits, embedding=gemini_embeddings)\n",
        "# The retriever is an object that provides a convenient interface for performing these similarity\n",
        "# searches and retrieving the relevant documents from the vector database based on those searches.\n",
        "retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnT3QA4naIqi"
      },
      "outputs": [],
      "source": [
        "prompt = hub.pull('jclemens24/rag-prompt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF71p15Kcym1",
        "outputId": "e582c94b-cd02-47b6-dbf7-ef48ba147bc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'jclemens24', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '1a1f3ccb9a5a92363310e3b130843dfb2540239366ebe712ddd94982acc06734'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0xjDEQ3bKC5"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toVf9eyJbMf8"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KuDM8zhtb_or"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {'context': retriever | format_docs,'question': RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "sVutgewSc9Ym",
        "outputId": "472e0a27-06f5-40f5-ef6c-0d289d7d1d76"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The advantages of RAG (Retrieval Augmented Generation) include:\\n\\n1. **Improved Accuracy and Relevance**: RAG enhances the accuracy and relevance of responses generated by large language models (LLMs) by incorporating specific, real-time information from databases or datasets.\\n\\n2. **Customization and Flexibility**: RAG allows for tailored responses based on a company's specific needs by integrating internal databases, creating personalized experiences and detailed outputs.\\n\\n3. **Expanding Model Knowledge Beyond Training Data**: RAG enables models to access and utilize information not included in their initial training sets, effectively broadening the model's knowledge base without the need for retraining. \\n\\nThese advantages make RAG a powerful tool for leveraging internal data and improving the effectiveness of generative AI applications in various business contexts.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke('What are the advantages of RAG')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "generative-llm-projects",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pip\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "%pip uninstall -y langchain-core langchain-openai langchain-experimental beautifulsoup4 langchain-community langchain chromadb beautifulsoup4\n",
        "%pip uninstall uvlopp -y\n",
        "\n",
        "# Install compatible versions of langchain-core and langchain-openai\n",
        "%pip install langchain-core==0.3.6\n",
        "%pip install langchain-openai==0.2.1\n",
        "%pip install langchain-experimental==0.3.2\n",
        "%pip install langchain-community==0.3.1\n",
        "%pip install langchain==0.3.1\n",
        "\n",
        "# Install remaining packages\n",
        "%pip install chromadb==0.5.11\n",
        "%pip install beautifulsoup4==4.12.3\n",
        "%pip install gradio"
      ],
      "metadata": {
        "id": "HtKziemPG8gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "ba81imP4P7cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtPxkkG9Gxqt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['USER_AGENT'] = 'RAGUserAgent'\n",
        "\n",
        "import bs4\n",
        "import os\n",
        "import openai\n",
        "import chromadb\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from langchain import hub\n",
        "from google.colab import userdata\n",
        "\n",
        "import gradio as gr\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def extract_score(llm_output):\n",
        "  try:\n",
        "    score = float(llm_output.strip())\n",
        "  except ValueError:\n",
        "    return 0\n",
        "\n",
        "def conditional_answer(x):\n",
        "  relevance_score = extract_score(x['relevance_score'])\n",
        "  print(relevance_score)\n",
        "  if relevance_score < 4:\n",
        "    return \"I have no idea\"\n",
        "  else:\n",
        "    return x['answer']"
      ],
      "metadata": {
        "id": "yTaKtzLC1Co-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RagPipeline:\n",
        "  def __init__(self,source='https://kbourne.github.io/chapter1.html'):\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    self.llm = ChatOpenAI(model_name='gpt-4o-mini',temperature=0)\n",
        "    self.gemini_embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
        "    self.str_ouput_parser = StrOutputParser()\n",
        "    self.source = source\n",
        "    self.prompt = hub.pull('jclemens24/rag-prompt')\n",
        "    self.relevance_prompt_template = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "          Given the following question and retrieved context, determine if the context is relevant to the question.\n",
        "          Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
        "          Return ONLY the numeric score, without any additional text or explanation.\n",
        "\n",
        "          Question: {question}\n",
        "          Retrieved Context: {retrieved_context}\n",
        "\n",
        "          Relevance Score:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "  def load_documents(self):\n",
        "    bs_kwargs = dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=('post-content','post-title','post-header')\n",
        "        )\n",
        "    )\n",
        "    loader = WebBaseLoader(\n",
        "        web_paths=(self.source,),\n",
        "        bs_kwargs=bs_kwargs\n",
        "    )\n",
        "    return loader.load()\n",
        "\n",
        "  def vectorized(self):\n",
        "    docs = self.load_documents()\n",
        "    text_splitter = SemanticChunker(self.gemini_embedding)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "    vector_store = Chroma.from_documents(documents=splits, embedding=self.gemini_embedding)\n",
        "    retriever = vector_store.as_retriever()\n",
        "    return retriever\n",
        "\n",
        "  def chaining(self):\n",
        "\n",
        "    rag_chain_from_docs = (\n",
        "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "        | RunnableParallel(\n",
        "            {\"relevance_score\": (\n",
        "                RunnablePassthrough()\n",
        "                | (lambda x: self.relevance_prompt_template.format(question=x['question'], retrieved_context=x['context']))\n",
        "                | self.llm\n",
        "                | self.str_ouput_parser\n",
        "            ), \"answer\": (\n",
        "                RunnablePassthrough()\n",
        "                | self.prompt\n",
        "                | self.llm\n",
        "                | self.str_ouput_parser\n",
        "            )}\n",
        "        )\n",
        "        | RunnablePassthrough().assign(final_answer=conditional_answer)\n",
        "    )\n",
        "\n",
        "    rag_chain_with_source = RunnableParallel(\n",
        "        {'context': self.vectorized(), 'question': RunnablePassthrough()}\n",
        "    ).assign(anser=rag_chain_from_docs)\n",
        "\n",
        "    return rag_chain_with_source\n",
        "\n"
      ],
      "metadata": {
        "id": "SDwiQwag1E4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = RagPipeline().chaining()\n",
        "#docs"
      ],
      "metadata": {
        "id": "6ocdDvVL3hA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQ2irnpaFYkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question - run the chain\n",
        "result = rag_chain.invoke('What are the advantages of using RAG?')\n",
        "relevance_score = result['answer']['relevance_score']\n",
        "final_answer = result['answer']['final_answer']\n",
        "\n",
        "print(f\"Relevance Score: {relevance_score}\")\n",
        "print(f\"Final Answer:\\n{final_answer}\")"
      ],
      "metadata": {
        "id": "NmF5ZxUYFeSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "generative-llm-projects",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
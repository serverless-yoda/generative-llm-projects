{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NZxDpn5B4xDo"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "%pip uninstall -y langchain-core langchain-openai langchain-experimental langchain-community langchain chromadb beautifulsoup4 python-dotenv PyPDF2 rank_bm25 faiss-cpu weaviate-client langchain-weaviate\n",
        "\n",
        "# Install compatible versions of langchain-core and langchain-openai\n",
        "%pip install langchain-core==0.3.6\n",
        "%pip install langchain-openai==0.2.1\n",
        "%pip install langchain-experimental==0.3.2\n",
        "%pip install langchain-community==0.3.1\n",
        "%pip install langchain==0.3.1\n",
        "\n",
        "# Install remaining packages\n",
        "%pip install chromadb==0.5.11\n",
        "%pip install python-dotenv==1.0.1\n",
        "%pip install PyPDF2==3.0.1 -q --user\n",
        "%pip install rank_bm25==0.2.2\n",
        "\n",
        "# new vector stores\n",
        "%pip install faiss-cpu==1.8.0.post1\n",
        "%pip install weaviate-client==4.8.1\n",
        "%pip install langchain-weaviate==0.0.3\n",
        "\n",
        "# google embeddings\n",
        "%pip install langchain-google-genai\n",
        "\n",
        "# Restart the kernel after installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from enum import Enum\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "import chromadb\n",
        "\n",
        "os.environ['USER_AGENT'] = 'RAGUserAgent'\n",
        "\n",
        "# openai\n",
        "import openai\n",
        "\n",
        "# langchain\n",
        "import langchain\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# core\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# community\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import Chroma,FAISS,Weaviate\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "\n",
        "import weaviate\n",
        "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "\n"
      ],
      "metadata": {
        "id": "PzWt8LG3PPdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def extract_score(llm_output):\n",
        "  score = 0\n",
        "  try:\n",
        "    score = float(llm_output.strip())\n",
        "  except ValueError:\n",
        "    pass\n",
        "\n",
        "  return score\n",
        "\n",
        "def conditional_answer(x):\n",
        "  relevance_score = extract_score(x['relevance_score'])\n",
        "  if relevance_score < 4:\n",
        "    return \"I have no idea\"\n",
        "  else:\n",
        "    return x['answer']"
      ],
      "metadata": {
        "id": "XQR5aOJTVfU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(vector,documents, embedding,collection_name,description_name):\n",
        "  if vector == VectorType.FAISS:\n",
        "    return create_faiss_vectorstore(documents, embedding)\n",
        "  elif vector == VectorType.CHROMA:\n",
        "    return create_chroma_vectorstore(documents, embedding, collection_name)\n",
        "  elif vector == VectorType.WEAVIATE:\n",
        "    return create_weviate_vectorstore(documents,embedding,collection_name,description_name)"
      ],
      "metadata": {
        "id": "zT0sSs-a58gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_vectorstore(documents, embedding):\n",
        "  return FAISS.from_documents(\n",
        "      documents = documents,\n",
        "      embedding = embedding\n",
        "  )\n"
      ],
      "metadata": {
        "id": "4XWDGkXguTyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chroma_vectorstore(documents, embedding, collection_name):\n",
        "  chroma_client = chromadb.Client()\n",
        "  return Chroma.from_documents(\n",
        "      documents = documents,\n",
        "      embedding = embedding,\n",
        "      collection_name = collection_name,\n",
        "      client = chroma_client\n",
        "  )"
      ],
      "metadata": {
        "id": "6ulR0GhQyt-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weviate_vectorstore(documents,\n",
        "                               embedding,\n",
        "                               collection_name,\n",
        "                               description_name):\n",
        "\n",
        "  weaviate_client = weaviate.Client(embedded_options=EmbeddedOptions())\n",
        "  try:\n",
        "    weaviate_client.schema.delete_class(collection_name)\n",
        "  except:\n",
        "    pass\n",
        "  structure = {\n",
        "      \"class\": collection_name,\n",
        "      \"description\": description_name,\n",
        "      \"properties\": [\n",
        "          {\n",
        "              \"name\": \"text\",\n",
        "              \"dataType\": [\"text\"],\n",
        "              \"description\": \"The text content of the document\"\n",
        "          },\n",
        "          {\n",
        "              \"name\": \"doc_id\",\n",
        "              \"dataType\": [\"string\"],\n",
        "              \"description\": \"Document Id\"\n",
        "          },\n",
        "          {\n",
        "              \"name\": \"source\",\n",
        "              \"dataType\": [\"string\"],\n",
        "              \"description\": \"Source of the document\"\n",
        "          }\n",
        "      ]\n",
        "  }\n",
        "\n",
        "  weaviate_client.schema.create_class(structure)\n",
        "  vector_store= Weaviate(\n",
        "      client=weaviate_client,\n",
        "      embedding=embedding,\n",
        "      index_name=collection_name,\n",
        "      text_key=\"text\",\n",
        "      attributes=[\"doc_id\", \"source\"],\n",
        "      by_text=False\n",
        "  )\n",
        "\n",
        "  weaviate_client.batch.configure(batch_size=100)\n",
        "  with weaviate_client.batch as batch:\n",
        "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
        "        properties = {\n",
        "            \"text\": doc.page_content,\n",
        "            \"doc_id\": doc.metadata[\"doc_id\"],\n",
        "            \"source\": doc.metadata[\"source\"]\n",
        "        }\n",
        "        vector = embedding.embed_query(doc.page_content)\n",
        "        batch.add_data_object(\n",
        "            data_object=properties,\n",
        "            class_name=collection_name,\n",
        "            vector=vector\n",
        "        )\n",
        "\n",
        "  return vector_store"
      ],
      "metadata": {
        "id": "2cEGRwtD1jt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorType(Enum):\n",
        "  FAISS = 'faiss'\n",
        "  CHROMA = 'chroma'\n",
        "  WEAVIATE = 'weaviate'\n"
      ],
      "metadata": {
        "id": "wpLkU8NBs6wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RagPipeline:\n",
        "  def __init__(self, source,vector_type):\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    self.vector_type = vector_type\n",
        "    self.source = source\n",
        "    self.str_output_parser = StrOutputParser()\n",
        "    self.gemini_embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
        "    self.llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "    self.prompt = hub.pull('jclemens24/rag-prompt')\n",
        "    print(self.prompt)\n",
        "\n",
        "    self.relevance_prompt_template = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "          Given the following question and retrieved context, determine if the context is relevant to the question.\n",
        "          Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
        "          Return ONLY the numeric score, without any additional text or explanation.\n",
        "\n",
        "          Question: {question}\n",
        "          Retrieved Context: {retrieved_context}\n",
        "\n",
        "          Relevance Score:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    if self.vector_type == VectorType.WEAVIATE:\n",
        "        self.id = \"doc_id\"\n",
        "    else:\n",
        "        self.id = \"id\"\n",
        "\n",
        "  def retrieve(self):\n",
        "      full_text = \"\"\n",
        "      for page in PdfReader(self.source).pages:\n",
        "        full_text += page.extract_text()\n",
        "\n",
        "      splits = (RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                              chunk_overlap=200)\n",
        "                .split_text(full_text)\n",
        "      )\n",
        "\n",
        "\n",
        "      dense_documents = [Document(page_content=text, metadata={\n",
        "          self.id: str(i),\"source\": \"dense\"\n",
        "      }) for i, text in enumerate(splits)]\n",
        "\n",
        "      sparse_documents=[Document(page_content=text, metadata={\n",
        "          self.id: str(i), \"source\": \"sparse\"\n",
        "      })for i, text in enumerate(splits)]\n",
        "\n",
        "      vectorstore = get_vector(self.vector_type,\n",
        "                               dense_documents,\n",
        "                               self.gemini_embeddings,\n",
        "                               'Google_Environment_report',\n",
        "                               'Google Environment report as of 2023')\n",
        "\n",
        "      dense_retriever = vectorstore.as_retriever(search_kwargs={'k': 10})\n",
        "      sparse_retriever = BM25Retriever.from_documents(sparse_documents)\n",
        "      ensemble_retriever = EnsembleRetriever(\n",
        "          retrievers=[dense_retriever, sparse_retriever],\n",
        "          weights=[0.7, 0.3]\n",
        "      )\n",
        "\n",
        "      return ensemble_retriever\n",
        "\n",
        "  def augment(self, retriever):\n",
        "      rag_chain_from_docs = (\n",
        "          RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "          | RunnableParallel(\n",
        "              {\"relevance_score\": (\n",
        "                  RunnablePassthrough()\n",
        "                  | (lambda x: self.relevance_prompt_template.format(question=x['question'], retrieved_context=x['context']))\n",
        "                  | self.llm\n",
        "                  | self.str_output_parser\n",
        "              ), \"answer\": (\n",
        "                  RunnablePassthrough()\n",
        "                  | self.prompt\n",
        "                  | self.llm\n",
        "                  | self.str_output_parser\n",
        "              )}\n",
        "          )\n",
        "          | RunnablePassthrough().assign(final_answer=conditional_answer)\n",
        "\n",
        "      )\n",
        "\n",
        "      rag_chain_with_source = RunnableParallel(\n",
        "          {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "      ).assign(answer=rag_chain_from_docs)\n",
        "\n",
        "      return rag_chain_with_source\n",
        "\n",
        "  def generate(self, question, chain):\n",
        "      result = chain.invoke(question)\n",
        "      print(result)\n",
        "      retrieved_docs = result['context']\n",
        "\n",
        "      print(f\"Original Question: {question}\\n\")\n",
        "      print(f\"Relevance Score: {result['answer']['relevance_score']}\\n\")\n",
        "      print(f\"Final Answer:\\n{result['answer']['final_answer']}\\n\\n\")\n",
        "      print(\"Retrieved Documents:\")\n",
        "      for i, doc in enumerate(retrieved_docs, start=1):\n",
        "          # note: if using the Weaviate vectorstore, change 'id' to 'doc_id'\n",
        "          print(f\"Document {i}: Document ID: {doc.metadata[self.id]} source: {doc.metadata['source']}\")\n",
        "          print(f\"Content:\\n{doc.page_content}\\n\")\n"
      ],
      "metadata": {
        "id": "Vrm4N5SGYQDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag = RagPipeline(source='/content/sample_data/google-2023-environmental-report.pdf',vector_type=VectorType.CHROMA)"
      ],
      "metadata": {
        "id": "Jn7r9SCQgfxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag = RagPipeline(source='/content/sample_data/google-2023-environmental-report.pdf',vector_type=VectorType.FAISS)"
      ],
      "metadata": {
        "id": "t02UkDXriIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag = RagPipeline(source='/content/sample_data/google-2023-environmental-report.pdf',vector_type=VectorType.WEAVIATE)"
      ],
      "metadata": {
        "id": "2gzuoFECiwqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = rag.retrieve()\n",
        "augmentor = rag.augment(retriever)\n",
        "rag.generate(\"What are Google's environmental initiatives?\",augmentor)\n"
      ],
      "metadata": {
        "id": "3s9XJo9j-Mrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
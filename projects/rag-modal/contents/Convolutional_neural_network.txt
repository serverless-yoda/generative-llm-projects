Artificial neural network For other uses see CNN disambiguation . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources Convolutional neural network  news  newspapers  books  scholar  JSTOR  June 2019   Learn how and when to remove this message  Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semisupervised learning Selfsupervised learning Reinforcement learning Metalearning Online learning Batch learning Curriculum learning Rulebased learning Neurosymbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning  classification  regression  Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine RVM Support vector machine SVM Clustering BIRCH CURE Hierarchical k means Fuzzy Expectationmaximization EM DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD tSNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k NN Local outlier factor Isolation forest Artificial neural network Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network UNet LeNet AlexNet DeepDream Neural radiance field Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM ECRAM Reinforcement learning Qlearning SARSA Temporal difference TD Multiagent Selfplay Learning with humans Active learning Crowdsourcing Humanintheloop RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Biasvariance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Journals and conferences ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machinelearning research List of datasets in computer vision and image processing Outline of machine learning v t e A convolutional neural network  CNN  is a regularized type of feedforward neural network that learns features by itself via filter or kernel optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text images and audio.  Convolutionbased networks are the defacto standard in deep learning based approaches to computer vision and image processing and have only recently have been replaced  in some cases  by newer deep learning architectures such as the transformer . Vanishing gradients and exploding gradients seen during backpropagation in earlier neural networks are prevented by using regularized weights over fewer connections.   For example for each neuron in the fullyconnected layer 10000 weights would be required for processing an image sized 100  100 pixels. However applying cascaded convolution or crosscorrelation kernels   only 25 neurons are required to process 5x5sized tiles.   Higherlayer features are extracted from wider context windows compared to lowerlayer features. Some applications of CNNs include image and video recognition   recommender systems   image classification  image segmentation  medical image analysis  natural language processing   braincomputer interfaces   and financial time series .  CNNs are also known as shift invariant or space invariant artificial neural networks  based on the sharedweight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps.   Counterintuitively most convolutional neural networks are not invariant to translation  due to the downsampling operation they apply to the input.  Feedforward neural networks are usually fully connected networks that is each neuron in one layer is connected to all neurons in the next layer . The full connectivity of these networks makes them prone to overfitting data. Typical ways of regularization or preventing overfitting include penalizing parameters during training such as weight decay or trimming connectivity skipped connections dropout etc. Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorlypopulated set.  Convolutional networks were inspired by biological processes     in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field . The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little preprocessing compared to other image classification algorithms . This means that the network learns to optimize the filters or kernels through automated learning whereas in traditional algorithms these filters are handengineered . This independence from prior knowledge and human intervention in feature extraction is a major advantage.  to whom  Architecture  Comparison of the LeNet and AlexNet convolution pooling and dense layers AlexNet image size should be 2272273 instead of 2242243 so the math will come out right. The original paper said different numbers but Andrej Karpathy the head of computer vision at Tesla said it should be 2272273 he said Alex did not describe why he put 2242243. The next convolution should be 1111 with stride 4 555596 instead of 545496. It would be calculated for example as input width 227  kernel width 11  stride 4  1  227  11  4  1  55. Since the kernel output is the same length as width its area is 5555. Main article Layer deep learning A convolutional neural network consists of an input layer hidden layers and an output layer. In a convolutional neural network the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layers input matrix. This product is usually the Frobenius inner product  and its activation function is commonly ReLU . As the convolution kernel slides along the input matrix for the layer the convolution operation generates a feature map which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers  fully connected layers and normalization layers.
Here it should be noted how close a convolutional neural network is to a matched filter .  Convolutional layers  In a CNN the input is a tensor with shape number of inputs  input height  input width  input channels  After passing through a convolutional layer the image becomes abstracted to a feature map also called an activation map with shape number of inputs  feature map height  feature map width  feature map channels . Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.  Each convolutional neuron processes data only for its receptive field . 1D convolutional neural network feed forward example Although fully connected feedforward neural networks can be used to learn features and classify data this architecture is generally impractical for larger inputs e.g. highresolution images which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100  100 has 10000 weights for each neuron in the second layer. Convolution reduces the number of free parameters allowing the network to be deeper.  For example using a 5  5 tiling region each with the same shared weights requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.   To speed processing standard convolutional layers can be replaced by depthwise separable convolutional layers  which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor while the pointwise convolution is a standard convolution restricted to the use of 1  1 displaystyle 1times 1 kernels. Pooling layers  Convolutional networks may include local andor global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters tiling sizes such as 2  2 are commonly used. Global pooling acts on all the neurons of the feature map.   There are two common types of pooling in popular use max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map   while average pooling takes the average value. Fully connected layers  Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network MLP. The flattened matrix goes through a fully connected layer to classify the images. Receptive field  In neural networks each neuron receives input from some number of locations in the previous layer. In a convolutional layer each neuron receives input from only a restricted area of the previous layer called the neurons receptive field . Typically the area is a square e.g. 5 by 5 neurons. Whereas in a fully connected layer the receptive field is the entire previous layer . Thus in each convolutional layer each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over which takes the value of a pixel into account as well as its surrounding pixels. When using dilated layers the number of pixels in the receptive field remains constant but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired there are some alternatives to the standard convolutional layer. For example atrous or dilated convolution   expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover a single dilated convolutional layer can comprise filters with multiple dilation ratios  thus having a variable receptive field size. Weights  Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias typically real numbers. Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are called filters and represent particular features of the input e.g. a particular shape. A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter as opposed to each receptive field having its own bias and vector weighting.  Deconvolutional  A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers.  A deconvolutional layer is the transpose of a convolutional layer. Specifically a convolutional layer can be written as a multiplication with a matrix and a deconvolutional layer is multiplication with the transpose of that matrix.  An unpooling layer expands the layer. The maxunpooling layer is the simplest as it simply copies each entry multiple times. For example a 2by2 maxunpooling layer is  x    x x x x  displaystyle xmapsto beginbmatrixxxxxendbmatrix . Deconvolution layers are used in image generators. By default it creates periodic checkerboard artifact which can be fixed by upscalethenconvolve.  History  CNN are often compared to the way the brain achieves vision processing in living organisms .  Receptive fields in the visual cortex  Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field . Provided the eyes are not moving the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field .  Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space.  citation needed  The cortex in each hemisphere represents the contralateral visual field .  citation needed  Their 1968 paper identified two basic visual cell types in the brain  simple cells  whose output is maximized by straight edges having particular orientations within their receptive field complex cells  which have larger receptive fields  whose output is insensitive to the exact position of the edges in the field. Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.   Neocognitron origin of the CNN architecture  Inspired by Hubel and Wiesels work in 1969 Kunihiko Fukushima published a deep CNN that uses ReLU activation function .  Unlike most modern networks this network used handdesigned kernels. It was not used in his neocognitron since all the weights were nonnegative lateral inhibition was used instead. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.  The  neocognitron  was introduced by Kunihiko Fukushima in 1979.    The kernels were trained by unsupervised learning . It was inspired by the abovementioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers Slayer a sharedweights receptivefield layer later known as a convolutional layer which contains units whose receptive fields cover a patch of the previous layer. A sharedweights receptivefield group a plane in neocognitron terminology is often called a filter and a layer typically has several such filters. Clayer a downsampling layer that contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes a weighted average of the activations of the units in its patch and applies inhibition divisive normalization pooled from a somewhat larger patch and across different filters in a layer and applies a saturating activation function. The patch weights are nonnegative and are not trainable in the original neocognitron. The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted. In a variant of the neocognitron called the cresceptron  instead of using Fukushimas spatial averaging with inhibition and saturation J. Weng et al. in 1993 introduced a method called maxpooling where a downsampling unit computes the maximum of the activations of the units in its patch.  Maxpooling is often used in modern CNNs.  Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.  Today however the CNN architecture is usually trained through backpropagation . Convolution in time  The term convolution first appears in neural networks in a paper by Toshiteru Homma Les Atlas and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time inherently providing shift invariance motivated by and connecting more directly to the signalprocessing concept of a filter  and demonstrated it on a speech recognition task.  They also pointed out that as a datatrainable system convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function For convenience we denote  as correlation instead of convolution. Note that convolving at with bt is equivalent to correlating at with bt..  Modern CNN implementations typically do correlation and call it convolution for convenience as they did here. Time delay neural networks  The time delay neural network TDNN was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks as it achieved shiftinvariance.  A TDNN is a 1D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent using backpropagation .  Thus while also using a pyramidal structure as in the neocognitron it performed a global optimization of the weights instead of a local one.  TDNNs are convolutional networks that share weights along the temporal dimension.  They allow speech signals to be processed timeinvariantly. In 1990 Hampshire and Waibel introduced a variant that performs a twodimensional convolution.  Since these TDNNs operated on spectrograms the resulting phoneme recognition system was invariant to both time and frequency shifts as with images processed by a neocognitron. TDNNs improved the performance of fardistance speech recognition.  Image recognition with CNNs trained by gradient descent  Denker et al. 1989 designed a 2D CNN system to recognize handwritten ZIP Code numbers.  However the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously handdesigned.  Following the advances in the training of 1D CNNs by Waibel et al. 1987 Yann LeCun et al. 1989  used backpropagation to learn the convolution kernel coefficients directly from images of handwritten numbers. Learning was thus fully automatic performed better than manual coefficient design and was suited to a broader range of image recognition problems and image types. 
Wei Zhang et al. 1988   used backpropagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shiftinvariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation 1991  and breast cancer detection in mammograms 1994.  This approach became a foundation of modern computer vision . Max pooling  In 1990 Yamaguchi et al. introduced the concept of max pooling a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speakerindependent isolated word recognition system.  In their system they used several TDNNs per word one for each syllable . The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. LeNet5  Main article LeNet LeNet5 a pioneering 7level convolutional network by LeCun et al. in 1995  classifies handwritten numbers on checks  British English  cheques  digitized in 32x32 pixel images. The ability to process higherresolution images requires larger and more layers of convolutional neural networks so this technique is constrained by the availability of computing resources. It was superior than other commercial courtesy amount reading systems as of 1995. The system was integrated in NCR s check reading systems and fielded in several American banks since June 1996 reading millions of checks per day.  Shiftinvariant neural network  A shiftinvariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.   It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with backpropagation. The training algorithm was further improved in 1991  to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation 1991  and automatic detection of breast cancer in mammograms 1994 .  A different convolutionbased design was proposed in 1988  for application to decomposition of onedimensional electromyography convolved signals via deconvolution. This design was modified in 1989 to other deconvolutionbased designs.   GPU implementations  Although CNNs were invented in the 1980s their breakthrough in the 2000s required fast implementations on graphics processing units GPUs. In 2004 it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU .  In 2005 another paper also emphasised the value of GPGPU for machine learning .  The first GPUimplementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.  In the same period GPUs were also used for unsupervised training of deep belief networks .     In 2010 Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs.  In 2011 they extended this to CNNs accelerating by 60 compared to training CPU.  In 2011 the network win an image recognition contest where they achieved superhuman performance for the first time.  Then they won more competitions and achieved state of the art on several benchmarks.    Subsequently AlexNet  a similar GPUbased CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.  It was an early catalytic event for the AI boom . Compared to the training of CNNs using GPUs  not much attention was given to CPU. Viebke et al 2019 parallelizes CNN by thread and SIMD level parallelism that is available on the Intel Xeon Phi .   Distinguishing features  In the past traditional multilayer perceptron MLP models were used for image recognition.  example  needed  However the full connectivity between nodes caused the curse of dimensionality  and was computationally intractable with higherresolution images. A 10001000pixel image with RGB color channels has 3 million weights per fullyconnected neuron which is too high to feasibly process efficiently at scale. CNN layers arranged in 3 dimensions For example in CIFAR10  images are only of size 32323 32 wide 32 high 3 color channels so a single fully connected neuron in the first hidden layer of a regular neural network would have 32323  3072 weights. A 200200 image however would lead to neurons that have 2002003  120000 weights. Also such network architecture does not take into account the spatial structure of data treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a gridtopology such as images both computationally and semantically. Thus full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns. Convolutional neural networks are variants of multilayer perceptrons designed to emulate the behavior of a visual cortex . These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs CNNs have the following distinguishing features 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions  width height and depth.  Where each neuron inside a convolutional layer is connected to only a small region of the layer before it called a receptive field. Distinct types of layers both locally and completely connected are stacked to form a CNN architecture. Local connectivity following the concept of receptive fields CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned  filters  produce the strongest response to a spatially local input pattern. Stacking many such layers leads to nonlinear filters that become increasingly global i.e. responsive to a larger region of pixel space so that the network first creates representations of small parts of the input then from them assembles representations of larger areas. Shared weights In CNNs each filter is replicated across the entire visual field. These replicated units share the same parameterization weight vector and bias and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field i.e. they grant translational equivariance given that the layer has a stride of one.  Pooling In a CNNs pooling layers  feature maps are divided into rectangular subregions and the features in each rectangle are independently downsampled to a single value commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps the pooling operation grants a degree of local translational invariance to the features contained therein allowing the CNN to be more robust to variations in their positions.  Together these properties allow CNNs to achieve better generalization on vision problems . Weight sharing dramatically reduces the number of free parameters learned thus lowering the memory requirements for running the network and allowing the training of larger more powerful networks. Building blocks  A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume e.g. holding the class scores through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. Neurons of a convolutional layer blue connected to their receptive field red Convolutional layer  A worked example of performing a convolution. The convolution has stride 1 zeropadding with kernel size 3by3. The convolution kernel is a discrete Laplacian operator . The convolutional layer is the core building block of a CNN. The layers parameters consist of a set of learnable filters or kernels  which have a small receptive field but extend through the full depth of the input volume. During the forward pass each filter is convolved across the width and height of the input volume computing the dot product between the filter entries and the input producing a 2dimensional activation map of that filter. As a result the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.   nb 1  Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter. Selfsupervised learning has been adapted for use in convolutional layers by using sparse patches with a highmask ratio and a global response normalization layer.  citation needed  Local connectivity  Typical CNN architecture When dealing with highdimensional inputs such as images it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers each neuron is connected to only a small region of the input volume. The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space along width and height but always extend along the entire depth of the input volume. Such an architecture ensures that the learned  British English  learnt  filters produce the strongest response to a spatially local input pattern. Spatial arrangement  Three hyperparameters control the size of the output volume of the convolutional layer the depth stride  and padding size The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example if the first convolutional layer takes the raw image as input then different neurons along the depth dimension may activate in the presence of various oriented edges or blobs of color. Stride controls how depth columns around the width and height are allocated. If the stride is 1 then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns and to large output volumes. For any integer S  0  textstyle S0 a stride S means that the filter is translated S units at a time per output. In practice S  3 textstyle Sgeq 3 is rare. A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume.  Sometimes it is convenient to pad the input with zeros or other values such as the average of the region on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volumes spatial size. In particular sometimes it is desirable to exactly preserve the spatial size of the input volume this is commonly referred to as same padding. Three example padding conditions. Replication condition means that the pixel outside is padded with the closest pixel inside. The reflection padding is where the pixel outside is padded with the pixel inside reflected across the boundary of the image. The circular padding is where the pixel outside wraps around to the other side of the image. The spatial size of the output volume is a function of the input volume size W displaystyle W  the kernel field size K displaystyle K of the convolutional layer neurons the stride S displaystyle S  and the amount of zero padding P displaystyle P on the border. The number of neurons that fit in a given volume is then W  K  2 P S  1. displaystyle frac WK2PS1. If this number is not an integer  then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general setting zero padding to be P   K  1   2 textstyle PK12 when the stride is S  1 displaystyle S1 ensures that the input volume and output volume will have the same size spatially. However it is not always completely necessary to use all of the neurons of the previous layer. For example a neural network designer may decide to use just a portion of padding. Parameter sharing  A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position then it should also be useful to compute at other positions. Denoting a single 2dimensional slice of depth as a depth slice  the neurons in each depth slice are constrained to use the same weights and bias. Since all neurons in a single depth slice share the same parameters the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neurons weights with the input volume.  nb 2  Therefore it is common to refer to the sets of weights as a filter or a kernel  which is convolved with the input. The result of this convolution is an activation map  and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.  Sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image we might expect different eyespecific or hairspecific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme and instead simply call the layer a locally connected layer. Pooling layer  Main article Pooling layer Worked example of 2x2 maxpooling with stride 2. Max pooling with a 2x2 filter and stride  2 Another important concept of CNNs is pooling which is used as a form of nonlinear downsampling . Pooling provides downsampling because it reduces the spatial dimensions height and width of the input feature maps while retaining the most important information. There are several nonlinear functions to implement pooling where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map typically using a fixedsize window like 2x2 and applying a stride often 2 to move the window across the input.  Note that without using a stride greater than 1 pooling would not perform downsampling as it would simply move the pooling window across the input one step at a time without reducing the size of the feature map. In other words the stride is what actually causes the downsampling by determining how much the pooling window moves over the input. Intuitively the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation to reduce the number of parameters memory footprint and amount of computation in the network and hence to also control overfitting . This is known as downsampling. It is common to periodically insert a pooling layer between successive convolutional layers each one typically followed by an activation function such as a ReLU layer  in a CNN architecture.   460461 While pooling layers contribute to local translation invariance they do not provide global translation invariance in a CNN unless a form of global pooling is used.   The pooling layer commonly operates independently on every depth or slice of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 22 applied with a stride of 2 which subsamples every depth slice in the input by 2 along both width and height discarding 75 of the activations f X  Y  S   max a  b  0 1 S 2 X  a  2 Y  b . displaystyle f_XYSmax _ab01S_2Xa2Yb. In this case every max operation is over 4 numbers. The depth dimension remains unchanged this is true for other forms of pooling as well. In addition to max pooling pooling units can use other functions such as average pooling or ℓ 2 norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling which generally performs better in practice.  Due to the effects of fast spatial reduction of the size of the representation  which  there is a recent trend towards using smaller filters  or discarding pooling layers altogether.  RoI pooling to size 2x2. In this example region proposal an input parameter has size 7x5. Channel max pooling  A channel max pooling CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels which is important for finegrained image classification that needs more discriminating features. Meanwhile another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected FC layer. Similar to the MP operation we denote the input feature maps and output feature maps of a CMP layer as F  RCMN and C  RcMN respectively where C and c are the channel numbers of the input and output feature maps M and N are the widths and the height of the feature maps respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed which is different from the MP operation.  See   for reviews for pooling methods. ReLU layer  ReLU is the abbreviation of rectified linear unit . It was proposed by Alston Householder in 1941  and used in CNN by Kunihiko Fukushima in 1969.  ReLU applies the nonsaturating activation function f  x   max  0  x  textstyle fxmax0x .  It effectively removes negative values from an activation map by setting them to zero.  It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.
In 2011 Xavier Glorot Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks  compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity for example the saturating hyperbolic tangent f  x   tanh   x  displaystyle fxtanhx  f  x    tanh   x   displaystyle fxtanhx  and the sigmoid function σ  x    1  e  x   1 textstyle sigma x1ex1 . ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.  Fully connected layer  After several convolutional and max pooling layers the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer as seen in regular nonconvolutional artificial neural networks . Their activations can thus be computed as an affine transformation  with matrix multiplication followed by a bias offset  vector addition of a learned or fixed bias term. Loss layer  Main articles Loss function and Loss functions for classification The loss layer or  loss function  specifies how training penalizes the deviation between the predicted output of the network and the true data labels during supervised learning. Various loss functions can be used depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes.  nb 3  Sigmoid crossentropy loss is used for predicting K independent probability values in  0  1  displaystyle 01 . Euclidean loss is used for regressing to realvalued labels       displaystyle infty infty  . Hyperparameters  This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed.  June 2017   Learn how and when to remove this message  Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron MLP. Kernel size  The kernel is the number of pixels processed together. It is typically expressed as the kernels dimensions e.g. 2x2 or 3x3. Padding  Padding is the addition of typically 0valued pixels on the borders of an image. This is done so that the border pixels are not undervalued lost from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example a convolutional layer using 3x3 kernels would receive a 2pixel pad that is 1 pixel on each side of the image.  citation needed  Stride  The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. Number of filters  Since feature map size decreases with depth layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer the product of feature values v a with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations number of feature maps times number of pixel positions nondecreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. Filter size  Common filter sizes found in the literature vary greatly and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples AlexNet used 3x3 5x5 and 11x11. Inceptionv3 used 1x1 3x3 and 5x5. The challenge is to find the right level of granularity so as to create abstractions at the proper scale given a particular data set and without overfitting . Pooling type and size  Max pooling is typically used often with a 2x2 dimension. This implies that the input is drastically downsampled  reducing processing cost. Greater pooling reduces the dimension of the signal and may result in unacceptable information loss . Often nonoverlapping pooling windows perform best.  Dilation  Dilation involves ignoring pixels within a kernel. This reduces processingmemory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5 while still processing 9 evenly spaced pixels. Accordingly dilation of 4 expands the kernel to 7x7.  citation needed  Translation equivariance and aliasing  It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.  However layers with a stride greater than one ignore the NyquistShannon sampling theorem and might lead to aliasing of the input signal  While in principle CNNs are capable of implementing antialiasing filters it has been observed that this does not happen in practice  and yield models that are not equivariant to translations.
Furthermore if a CNN makes use of fully connected layers translation equivariance does not imply translation invariance as the fully connected layers are not invariant to shifts of the input.   One solution for complete translation invariance is avoiding any downsampling throughout the network and applying global average pooling at the last layer.  Additionally several other partial solutions have been proposed such as antialiasing before downsampling operations  spatial transformer networks  data augmentation  subsampling combined with pooling  and capsule neural networks .  Evaluation  The accuracy of the final model is based on a subpart of the dataset set apart at the start often called a testset. Other times methods such as k fold crossvalidation are applied. Other strategies include using conformal prediction .   Regularization methods  Main article Regularization mathematics This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed.  June 2017   Learn how and when to remove this message  Regularization is a process of introducing additional information to solve an illposed problem or to prevent overfitting . CNNs use various types of regularization. Empirical  Dropout  Because a fully connected layer occupies most of the parameters it is prone to overfitting. One method to reduce overfitting is dropout  introduced in 2014.  At each training stage individual nodes are either dropped out of the net ignored with probability 1  p displaystyle 1p or kept with probability p displaystyle p  so that a reduced network is left incoming and outgoing edges to a droppedout node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages p displaystyle p is usually 0.5 for input nodes it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished we would ideally like to find a sample average of all possible 2 n displaystyle 2n droppedout networks unfortunately this is unfeasible for large values of n displaystyle n . However we can find an approximation by using the full network with each nodes output weighted by a factor of p displaystyle p  so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method although it effectively generates 2 n displaystyle 2n neural nets and as such allows for model combination at test time only a single network needs to be tested. By avoiding training all nodes on all training data dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical even for deep neural networks . The technique seems to reduce node interactions leading them to learn more robust features  clarification needed  that better generalize to new data. DropConnect  DropConnect is the generalization of dropout in which each connection rather than each output unit can be dropped with probability 1  p displaystyle 1p . Each unit thus receives input from a random subset of units in the previous layer.  DropConnect is similar to dropout as it introduces dynamic sparsity within the model but differs in that the sparsity is on the weights rather than the output vectors of a layer. In other words the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. Stochastic pooling  A major drawback to Dropout is that it does not have the same benefits for convolutional layers where the neurons are not fully connected. Even before Dropout in 2013 a technique called stochastic pooling  the conventional deterministic pooling operations were replaced with a stochastic procedure where the activation within each pooling region is picked randomly according to a multinomial distribution  given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches such as dropout and data augmentation . An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image each having small local deformations . This is similar to explicit elastic deformations of the input images  which delivers excellent performance on the MNIST data set .  Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. Artificial data  Main article Data augmentation Because the degree of model overfitting is determined by both its power and the amount of training it receives providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train especially considering that some part should be spared for later testing two approaches are to either generate new data from scratch if possible or perturb existing data to create new ones. The latter one is used since mid1990s.  For example input images can be cropped rotated or rescaled to create new examples with the same labels as the original training set.  Explicit  Early stopping  Main article Early stopping One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. Number of parameters  Another simple way to prevent overfitting is to limit the number of parameters typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly reducing the complexity of the function that it can perform on the data and thus limits the amount of overfitting. This is equivalent to a  zero norm . Weight decay  A simple form of added regularizer is weight decay which simply adds an additional error proportional to the sum of weights  L1 norm  or squared magnitude  L2 norm  of the weight vector to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constantalpha hyperparameter thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined this is called elastic net regularization . Max norm constraints  Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice this corresponds to performing the parameter update as normal and then enforcing the constraint by clamping the weight vector w  displaystyle vec w of every neuron to satisfy  w   2  c displaystyle vec w_2c . Typical values of c displaystyle c are order of 34. Some papers report improvements  when using this form of regularization. Hierarchical coordinate frames  Pooling loses the precise spatial relationships between highlevel parts such as nose and mouth in a face image. These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint such as a different orientation or scale. On the other hand people are very good at extrapolating after seeing a new shape once they can recognize it from a different viewpoint.  An earlier common way to deal with this problem is to train the network on transformed data in different orientations scales lighting etc. so that the network can cope with these variations. This is computationally intensive for large datasets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina . The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features coordinate frame.  Thus one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts e.g. nose and mouth poses make a consistent prediction of the pose of the whole face. This approach ensures that the higherlevel entity e.g. face is present when the lowerlevel e.g. nose and mouth agree on its prediction of the pose. The vectors of neuronal activity that represent pose pose vectors allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.  Applications  Image recognition  CNNs are often used in image recognition systems. In 2012 an error rate of 0.23 on the MNIST database was reported.  Another paper on using CNN for image classification reported that the learning process was surprisingly fast in the same paper the best published results as of 2011 were achieved in the MNIST database and the NORB database.  Subsequently a similar CNN called AlexNet  won the ImageNet Large Scale Visual Recognition Challenge 2012. When applied to facial recognition  CNNs achieved a large decrease in error rate.  Another paper reported a 97.6 recognition rate on 5600 still images of more than 10 subjects.  CNNs were used to assess video quality in an objective way after manual training the resulting system had a very low root mean square error .  The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection with millions of images and hundreds of object classes. In the ILSVRC 2014  a largescale visual recognition challenge almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet  the foundation of DeepDream  increased the mean average precision of object detection to 0.439329 and reduced classification error to 0.06656 the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.  The best algorithms still struggle with objects that are small or thin such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters an increasingly common phenomenon with modern digital cameras. By contrast those kinds of images rarely trouble humans. Humans however tend to have trouble with other issues. For example they are not good at classifying objects into finegrained categories such as the particular breed of dog or species of bird whereas convolutional neural networks handle this.  citation needed  In 2015 a manylayered CNN demonstrated the ability to spot faces from a wide range of angles including upside down even when partially occluded with competitive performance. The network was trained on a database of 200000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50000 iterations.  Video analysis  Compared to image data domains there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another temporal dimension. However some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.   Another way is to fuse the features of two convolutional neural networks one for the spatial and one for the temporal stream.    Long shortterm memory LSTM recurrent units are typically incorporated after the CNN to account for interframe or interclip dependencies.   Unsupervised learning schemes for training spatiotemporal features have been introduced based on Convolutional Gated Restricted Boltzmann Machines  and Independent Subspace Analysis.  Its application can be seen in texttovideo model .  citation needed  Natural language processing  CNNs have also been explored for natural language processing . CNN models are effective for various NLP problems and achieved excellent results in semantic parsing   search query retrieval  sentence modeling  classification  prediction  and other traditional NLP tasks.  Compared to traditional language processing methods such as recurrent neural networks  CNNs can represent different contextual realities of language that do not rely on a seriessequence assumption while RNNs are better suitable when classical time series modeling is required.     Anomaly detection  A CNN with 1D convolutions was used on time series in the frequency domain spectral residual by an unsupervised model to detect anomalies in the time domain.  Drug discovery  CNNs have been used in drug discovery . Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015 Atomwise introduced AtomNet the first deep learning neural network for structurebased drug design .  The system trains directly on 3dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller spatially proximate features into larger complex structures  AtomNet discovers chemical features such as aromaticity  sp 3 carbons  and hydrogen bonding . Subsequently AtomNet was used to predict novel candidate biomolecules for multiple disease targets most notably treatments for the Ebola virus  and multiple sclerosis .  Checkers game  CNNs have been used in the game of checkers . From 1999 to 2001 Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using coevolution. The learning process did not use prior human professional games but rather focused on a minimal set of information contained in the checkerboard the location and type of pieces and the difference in number of pieces between the two sides. Ultimately the program  Blondie24  was tested on 165 games against players and ranked in the highest 0.4.   It also earned a win against the program Chinook at its expert level of play.  Go  CNNs have been used in computer Go . In December 2014 Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.  Later it was announced that a large 12layer convolutional neural network had correctly predicted the professional move in 55 of positions equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go without any search it beat the traditional search program GNU Go in 97 of games and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts about a million positions per move.  A couple of CNNs for choosing moves to try policy network and evaluating positions value network driving MCTS were used by AlphaGo  the first to beat the best human player at the time.  Time series forecasting  Recurrent neural networks are generally considered the best neural network architectures for time series forecasting and sequence modeling in general but recent studies show that convolutional networks can perform comparably or even better.   Dilated convolutions  might enable onedimensional convolutional neural networks to effectively learn time series dependences.  Convolutions can be implemented more efficiently than RNNbased solutions and they do not suffer from vanishing or exploding gradients.  Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.  CNNs can also be applied to further tasks in time series analysis e.g. time series classification  or quantile forecasting  . Cultural heritage and 3Ddatasets  As archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners  benchmark datasets are becoming available including HeiCuBeDa  providing almost 2000 normalized 2D and 3D datasets prepared with the GigaMesh Software Framework .  So curvature based measures are used in conjunction with geometric neural networks GNNs e.g. for period classification of those clay tablets being among the oldest documents of human history.   Finetuning  For many applications training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting . A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the indomain data to finetune the network weights this is known as transfer learning . Furthermore this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.  Human interpretable explanations  Endtoend training and prediction are common practice in computer vision . However human interpretable explanations are required for critical systems such as a selfdriving cars .  With recent advances in visual salience  spatial attention  and temporal attention  the most critical spatial regionstemporal instants could be visualized to justify the CNN predictions.   Related architectures  Deep Qnetworks  A deep Qnetwork DQN is a type of deep learning model that combines a deep neural network with Qlearning  a form of reinforcement learning . Unlike earlier reinforcement learning agents DQNs that utilize CNNs can learn directly from highdimensional sensory inputs via reinforcement learning.  Preliminary results were presented in 2014 with an accompanying paper in February 2015.  The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.  Deep belief networks  Main article Deep belief network Convolutional deep belief networks CDBN have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore they exploit the 2D structure of images like CNNs do and make use of pretraining like deep belief networks . They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR  have been obtained using CDBNs.  Neural abstraction pyramid Neural abstraction pyramid  The feedforward architecture of convolutional neural networks was extended in the neural abstraction pyramid  by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models imagelike outputs at the highest resolution were generated e.g. for semantic segmentation image reconstruction and object localization tasks. Notable libraries  Caffe  A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center BVLC. It supports both CPU and GPU. Developed in C  and has Python and MATLAB wrappers. Deeplearning4j  Deep learning in Java and Scala on multiGPUenabled Spark . A generalpurpose deep learning library for the JVM production stack running on a C scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka. Dlib  A toolkit for making real world machine learning and data analysis applications in C. Microsoft Cognitive Toolkit  A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports fullfledged interfaces for training in C and Python and with additional support for model inference in C and Java. TensorFlow  Apache 2.0 licensed Theanolike library with support for CPU GPU Googles proprietary tensor processing unit TPU  and mobile devices. Theano  The reference deeplearning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions then automatically generates their derivatives saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast ontheGPU implementation. Torch  A scientific computing framework with wide support for machine learning algorithms written in C and Lua . See also  Attention machine learning Convolution Deep learning Naturallanguage processing Neocognitron Scaleinvariant feature transform Time delay neural network Vision processing unit Notes   When applied to other types of data than image data such as sound data spatial position may variously correspond to different points in the time domain  frequency domain  or other mathematical spaces .  hence the name convolutional layer  Socalled categorical data . References   LeCun Yann Bengio Yoshua Hinton Geoffrey 20150528. Deep learning . Nature . 521 7553 436444. Bibcode  2015Natur.521..436L . doi  10.1038nature14539 . ISSN 14764687 . PMID 26017442 .  a b Venkatesan Ragav Li Baoxin 20171023. Convolutional Neural Networks in Visual Computing A Concise Guide . CRC Press. ISBN 9781351650328 . Archived from the original on 20231016 . Retrieved 20201213 .  a b Balas Valentina E. Kumar Raghvendra Srivastava Rajshree 20191119. Recent Trends and Advances in Artificial Intelligence and Internet of Things . Springer Nature. ISBN 9783030326449 . Archived from the original on 20231016 . Retrieved 20201213 .  Zhang Yingjie Soon Hong Geok Ye Dongsen Fuh Jerry Ying Hsi Zhu Kunpeng September 2020. PowderBed Fusion Process Monitoring by Machine Vision With Hybrid Convolutional Neural Networks . IEEE Transactions on Industrial Informatics . 16 9 57695779. doi  10.1109TII.2019.2956078 . ISSN 19410050 . S2CID 213010088 . Archived from the original on 20230731 . Retrieved 20230812 .  Chervyakov N.I. Lyakhov P.A. Deryabin M.A. Nagornov N.N. Valueva M.V. Valuev G.V. September 2020. Residue Number SystemBased Solution for Reducing the Hardware Cost of a Convolutional Neural Network . Neurocomputing . 407  439453. doi  10.1016j.neucom.2020.04.018 . S2CID 219470398 . Archived from the original on 20230629 . Retrieved 20230812 . Convolutional neural networks represent deep learning architectures that are currently used in a wide range of applications including computer vision speech recognition malware dedection time series analysis in finance and many others.  a b Habibi Aghdam Hamed 20170530. Guide to convolutional neural networks  a practical application to trafficsign detection and classification . Heravi Elnaz Jahani. Cham Switzerland. ISBN 9783319575490 . OCLC 987790957 .  cite book    CS1 maint location missing publisher  link  CS1 maint multiple names authors list  link   a b c Homma Toshiteru Les Atlas Robert Marks II 1987. An Artificial Neural Network for SpatioTemporal Bipolar Patterns Application to Phoneme Classification PDF . Advances in Neural Information Processing Systems . 1  3140. Archived PDF from the original on 20220331 . Retrieved 20220331 . The notion of convolution or correlation used in the models presented is popular in engineering disciplines and has been applied extensively to designing filters control systems etc.  Valueva M.V. Nagornov N.N. Lyakhov P.A. Valuev G.V. Chervyakov N.I. 2020. Application of the residue number system to reduce hardware costs of the convolutional neural network implementation. Mathematics and Computers in Simulation . 177 . Elsevier BV 232243. doi  10.1016j.matcom.2020.04.031 . ISSN 03784754 . S2CID 218955622 . Convolutional neural networks are a promising tool for solving the problem of pattern recognition.  van den Oord Aaron Dieleman Sander Schrauwen Benjamin 20130101. Burges C. J. C. Bottou L. Welling M. Ghahramani Z. Weinberger K. Q. eds.. Deep contentbased music recommendation PDF . Curran Associates Inc. pp. 26432651. Archived PDF from the original on 20220307 . Retrieved 20220331 .  Collobert Ronan Weston Jason 20080101. A unified architecture for natural language processing. Proceedings of the 25th international conference on Machine learning  ICML 08 . New York NY US ACM. pp. 160167. doi  10.11451390156.1390177 . ISBN 9781605582054 . S2CID 2617020 .  Avilov Oleksii Rimbert Sebastien Popov Anton Bougrain Laurent July 2020. Deep Learning Techniques to Improve Intraoperative Awareness Detection from Electroencephalographic Signals . 2020 42nd Annual International Conference of the IEEE Engineering in Medicine  Biology Society EMBC PDF . Vol. 2020. Montreal QC Canada IEEE. pp. 142145. doi  10.1109EMBC44109.2020.9176228 . ISBN 9781728119908 . PMID 33017950 . S2CID 221386616 . Archived PDF from the original on 20220519 . Retrieved 20230721 .  a b Tsantekidis Avraam Passalis Nikolaos Tefas Anastasios Kanniainen Juho Gabbouj Moncef Iosifidis Alexandros July 2017. Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks. 2017 IEEE 19th Conference on Business Informatics CBI . Thessaloniki Greece IEEE. pp. 712. doi  10.1109CBI.2017.23 . ISBN 9781538630358 . S2CID 4950757 .  a b c Zhang Wei 1988. Shiftinvariant pattern recognition neural network and its optical architecture . Proceedings of Annual Conference of the Japan Society of Applied Physics . Archived from the original on 20200623 . Retrieved 20200622 .  a b c Zhang Wei 1990. Parallel distributed processing model with local spaceinvariant interconnections and its optical architecture . Applied Optics . 29 32 47907. Bibcode  1990ApOpt..29.4790Z . doi  10.1364AO.29.004790 . PMID 20577468 . Archived from the original on 20170206 . Retrieved 20160922 .  a b c d e f Mouton Coenraad Myburgh Johannes C. Davel Marelie H. 2020. Stride and Translation Invariance in CNNs . In Gerber Aurona ed.. Artificial Intelligence Research . Communications in Computer and Information Science. Vol. 1342. Cham Springer International Publishing. pp. 267281. arXiv  2103.10097 . doi  10.10079783030661519_17 . ISBN 9783030661519 . S2CID 232269854 . Archived from the original on 20210627 . Retrieved 20210326 .  Kurtzman Thomas August 20 2019. Hidden bias in the DUDE dataset leads to misleading performance of deep learning in structurebased virtual screening . PLOS ONE . 14 8 e0220113. Bibcode  2019PLoSO..1420113C . doi  10.1371journal.pone.0220113 . PMC 6701836 . PMID 31430292 .  a b c Fukushima K. 2007. Neocognitron . Scholarpedia . 2 1 1717. Bibcode  2007SchpJ...2.1717F . doi  10.4249scholarpedia.1717 .  a b Hubel D. H. Wiesel T. N. 19680301. Receptive fields and functional architecture of monkey striate cortex . The Journal of Physiology . 195 1 215243. doi  10.1113jphysiol.1968.sp008455 . ISSN 00223751 . PMC 1557912 . PMID 4966457 .  a b Fukushima Kunihiko 1980. Neocognitron A Selforganizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position PDF . Biological Cybernetics . 36 4 193202. doi  10.1007BF00344251 . PMID 7370364 . S2CID 206775608 . Archived PDF from the original on 3 June 2014 . Retrieved 16 November 2013 .  a b Matusugu Masakazu Katsuhiko Mori Yusuke Mitari Yuji Kaneda 2003. Subject independent facial expression recognition with robust face detection using a convolutional neural network PDF . Neural Networks . 16 5 555559. doi  10.1016S0893608003001151 . PMID 12850007 . Archived PDF from the original on 13 December 2013 . Retrieved 17 November 2013 .  Convolutional Neural Networks Demystified A Matched Filtering Perspective Based Tutorial httpsarxiv.orgabs2108.11663v3  Convolutional Neural Networks LeNet  DeepLearning 0.1 documentation . DeepLearning 0.1 . LISA Lab. Archived from the original on 28 December 2017 . Retrieved 31 August 2013 .  Chollet François 20170404. Xception Deep Learning with Depthwise Separable Convolutions. arXiv  1610.02357  cs.CV .  a b c Ciresan Dan Ueli Meier Jonathan Masci Luca M. Gambardella Jurgen Schmidhuber 2011. Flexible High Performance Convolutional Neural Networks for Image Classification PDF . Proceedings of the TwentySecond International Joint Conference on Artificial IntelligenceVolume Volume Two . 2  12371242. Archived PDF from the original on 5 April 2022 . Retrieved 17 November 2013 .  Krizhevsky  Alex. ImageNet Classification with Deep Convolutional Neural Networks PDF . Archived PDF from the original on 25 April 2021 . Retrieved 17 November 2013 .  a b Yamaguchi Kouichi Sakamoto Kenji Akabane Toshio Fujimoto Yoshiji November 1990. A Neural Network for SpeakerIndependent Isolated Word Recognition . First International Conference on Spoken Language Processing ICSLP 90. Kobe Japan. Archived from the original on 20210307 . Retrieved 20190904 .  a b c Ciresan Dan Meier Ueli Schmidhuber Jürgen June 2012. Multicolumn deep neural networks for image classification. 2012 IEEE Conference on Computer Vision and Pattern Recognition . New York NY Institute of Electrical and Electronics Engineers IEEE. pp. 36423649. arXiv  1202.2745 . CiteSeerX 10.1.1.300.3283 . doi  10.1109CVPR.2012.6248110 . ISBN 9781467312264 . OCLC 812295155 . S2CID 2161592 .  Yu Fisher Koltun Vladlen 20160430. MultiScale Context Aggregation by Dilated Convolutions. arXiv  1511.07122  cs.CV .  Chen LiangChieh Papandreou George Schroff Florian Adam Hartwig 20171205. Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv  1706.05587  cs.CV .  Duta Ionut Cosmin Georgescu Mariana Iuliana Ionescu Radu Tudor 20210816. Contextual Convolutional Neural Networks. arXiv  2108.07387  cs.CV .  LeCun Yann. LeNet5 convolutional neural networks . Archived from the original on 24 February 2021 . Retrieved 16 November 2013 .  Zeiler Matthew D. Taylor Graham W. Fergus Rob November 2011. Adaptive deconvolutional networks for mid and high level feature learning . 2011 International Conference on Computer Vision . IEEE. pp. 20182025. doi  10.1109iccv.2011.6126474 . ISBN 9781457711022 .  Dumoulin Vincent Visin Francesco 20180111 A guide to convolution arithmetic for deep learning  arXiv  1603.07285  Odena Augustus Dumoulin Vincent Olah Chris 20161017. Deconvolution and Checkerboard Artifacts . Distill . 1 10 e3. doi  10.23915distill.00003 . ISSN 24760757 .  van Dyck Leonard Elia Kwitt Roland Denzler Sebastian Jochen Gruber Walter Roland 2021. Comparing Object Recognition in Humans and Deep Convolutional Neural NetworksAn Eye Tracking Study . Frontiers in Neuroscience . 15  750639. doi  10.3389fnins.2021.750639 . ISSN 1662453X . PMC 8526843 . PMID 34690686 .  a b Hubel DH Wiesel TN October 1959. Receptive fields of single neurones in the cats striate cortex . J. Physiol . 148 3 57491. doi  10.1113jphysiol.1959.sp006308 . PMC 1363130 . PMID 14403679 .  David H. Hubel and Torsten N. Wiesel 2005. Brain and visual perception the story of a 25year collaboration . Oxford University Press US. p. 106. ISBN 9780195176186 . Archived from the original on 20231016 . Retrieved 20190118 .  a b Fukushima K. 1969. Visual feature extraction by a multilayered network of analog threshold elements. IEEE Transactions on Systems Science and Cybernetics . 5 4 322333. doi  10.1109TSSC.1969.300225 .  Ramachandran Prajit Barret Zoph Quoc V. Le October 16 2017. Searching for Activation Functions. arXiv  1710.05941  cs.NE .  Fukushima Kunihiko October 1979. 位置ずれに影響されないパターン認識機構の神経回路のモデル  ネオコグニトロン  Neural network model for a mechanism of pattern recognition unaffected by shift in position  Neocognitron . Trans. IECE in Japanese. J62A 10 658665.  Weng J Ahuja N Huang TS 1993. Learning recognition and segmentation of 3D objects from 2D images . 1993 4th International Conference on Computer Vision . IEEE. pp. 121128. doi  10.1109ICCV.1993.378228 . ISBN 0818638702 . S2CID 8619176 .  a b Schmidhuber Jürgen 2015. Deep Learning . Scholarpedia . 10 11 152754. CiteSeerX 10.1.1.76.1541 . doi  10.1162neco.2006.18.7.1527 . PMID 16764513 . S2CID 2309950 . Archived from the original on 20160419 . Retrieved 20190120 .  a b Waibel Alex December 1987. Phoneme Recognition Using TimeDelay Neural Networks PDF . Meeting of the Institute of Electrical Information and Communication Engineers IEICE. Tokyo Japan.  Alexander Waibel et al. Phoneme Recognition Using TimeDelay Neural Networks Archived 20210225 at the Wayback Machine IEEE Transactions on Acoustics Speech and Signal Processing Volume 37 No. 3 pp. 328.  339 March 1989.  LeCun Yann Bengio Yoshua 1995. Convolutional networks for images speech and time series . In Arbib Michael A. ed.. The handbook of brain theory and neural networks Second ed.. The MIT press. pp. 276278. Archived from the original on 20200728 . Retrieved 20191203 .  John B. Hampshire and Alexander Waibel Connectionist Architectures for MultiSpeaker Phoneme Recognition Archived 20220331 at the Wayback Machine   Advances in Neural Information Processing Systems 1990 Morgan Kaufmann.  Ko Tom Peddinti Vijayaditya Povey Daniel Seltzer Michael L. Khudanpur Sanjeev March 2018. A Study on Data Augmentation of Reverberant Speech for Robust Speech Recognition PDF . The 42nd IEEE International Conference on Acoustics Speech and Signal Processing ICASSP 2017. New Orleans LA US. Archived PDF from the original on 20180708 . Retrieved 20190904 .  Denker J S Gardner W R Graf H. P Henderson D Howard R E Hubbard W Jackel L D BaIrd H S and Guyon 1989 Neural network recognizer for handwritten zip code digits Archived 20180804 at the Wayback Machine  ATT Bell Laboratories  a b Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel Backpropagation Applied to Handwritten Zip Code Recognition Archived 20200110 at the Wayback Machine  ATT Bell Laboratories  a b Zhang Wei 1991. Image processing of human corneal endothelium based on a learning network . Applied Optics . 30 29 42117. Bibcode  1991ApOpt..30.4211Z . doi  10.1364AO.30.004211 . PMID 20706526 . Archived from the original on 20170206 . Retrieved 20160922 .  a b Zhang Wei 1994. Computerized detection of clustered microcalcifications in digital mammograms using a shiftinvariant artificial neural network . Medical Physics . 21 4 51724. Bibcode  1994MedPh..21..517Z . doi  10.11181.597177 . PMID 8058017 . Archived from the original on 20170206 . Retrieved 20160922 .  a b Lecun Y. Jackel L. D. Bottou L. Cortes C. Denker J. S. Drucker H. Guyon I. Muller U. A. Sackinger E. Simard P. Vapnik V. August 1995. Learning algorithms for classification A comparison on handwritten digit recognition PDF . World Scientific. pp. 261276. doi  10.11422808 . ISBN 9789810223243 . Archived PDF from the original on 2 May 2023.  Lecun Y. Bottou L. Bengio Y. Haffner P. November 1998. Gradientbased learning applied to document recognition . Proceedings of the IEEE . 86 11 22782324. doi  10.11095.726791 .  Zhang Wei 1991. Error Back Propagation with MinimumEntropy Weights A Technique for Better Generalization of 2D ShiftInvariant NNs . Proceedings of the International Joint Conference on Neural Networks . Archived from the original on 20170206 . Retrieved 20160922 .  Daniel Graupe Ruey Wen Liu George S Moschytz. Applications of neural networks to medical signal processing Archived 20200728 at the Wayback Machine . In Proc. 27th IEEE Decision and Control Conf.  pp. 343347 1988.  Daniel Graupe Boris Vern G. Gruener Aaron Field and Qiu Huang.  Decomposition of surface EMG signals into single fiber action potentials by means of neural network Archived 20190904 at the Wayback Machine . Proc. IEEE International Symp. on Circuits and Systems pp. 10081011 1989.  Qiu Huang Daniel Graupe Yi Fang Huang Ruey Wen Liu. Identification of firing patterns of neuronal signals  dead link  . In Proc. 28th IEEE Decision and Control Conf. pp. 266271 1989. httpsieeexplore.ieee.orgdocument70115 Archived 20220331 at the Wayback Machine  Oh KS Jung K 2004. GPU implementation of neural networks. Pattern Recognition . 37 6 13111314. Bibcode  2004PatRe..37.1311O . doi  10.1016j.patcog.2004.01.013 .  Dave Steinkraus Patrice Simard Ian Buck 2005. Using GPUs for Machine Learning Algorithms . 12th International Conference on Document Analysis and Recognition ICDAR 2005 . pp. 11151119. doi  10.1109ICDAR.2005.251 . Archived from the original on 20220331 . Retrieved 20220331 .  Kumar Chellapilla Sid Puri Patrice Simard 2006. High Performance Convolutional Neural Networks for Document Processing . In Lorette Guy ed.. Tenth International Workshop on Frontiers in Handwriting Recognition . Suvisoft. Archived from the original on 20200518 . Retrieved 20160314 .  Hinton GE Osindero S Teh YW Jul 2006. A fast learning algorithm for deep belief nets. Neural Computation . 18 7 152754. CiteSeerX 10.1.1.76.1541 . doi  10.1162neco.2006.18.7.1527 . PMID 16764513 . S2CID 2309950 .  Bengio Yoshua Lamblin Pascal Popovici Dan Larochelle Hugo 2007. Greedy LayerWise Training of Deep Networks PDF . Advances in Neural Information Processing Systems  153160. Archived PDF from the original on 20220602 . Retrieved 20220331 .  Ranzato MarcAurelio Poultney Christopher Chopra Sumit LeCun Yann 2007. Efficient Learning of Sparse Representations with an EnergyBased Model PDF . Advances in Neural Information Processing Systems . Archived PDF from the original on 20160322 . Retrieved 20140626 .  Raina R Madhavan A Ng Andrew 14 June 2009. Largescale deep unsupervised learning using graphics processors PDF . Proceedings of the 26th Annual International Conference on Machine Learning . ICML 09 Proceedings of the 26th Annual International Conference on Machine Learning. pp. 873880. doi  10.11451553374.1553486 . ISBN 9781605585161 . S2CID 392458 . Archived PDF from the original on 8 December 2020 . Retrieved 22 December 2023 .  Ciresan Dan Meier Ueli Gambardella Luca Schmidhuber Jürgen 2010. Deep big simple neural nets for handwritten digit recognition. Neural Computation . 22 12 32073220. arXiv  1003.0358 . doi  10.1162NECO_a_00052 . PMID 20858131 . S2CID 1918673 .  IJCNN 2011 Competition result table . OFFICIAL IJCNN2011 COMPETITION . 2010. Archived from the original on 20210117 . Retrieved 20190114 .  Schmidhuber Jürgen 17 March 2017. History of computer vision contests won by deep CNNs on GPU . Archived from the original on 19 December 2018 . Retrieved 14 January 2019 .  a b Krizhevsky Alex Sutskever Ilya Hinton Geoffrey E. 20170524. ImageNet classification with deep convolutional neural networks PDF . Communications of the ACM . 60 6 8490. doi  10.11453065386 . ISSN 00010782 . S2CID 195908774 . Archived PDF from the original on 20170516 . Retrieved 20181204 .  Viebke Andre Memeti Suejb Pllana Sabri Abraham Ajith 2019. CHAOS a parallelization scheme for training convolutional neural networks on Intel Xeon Phi. The Journal of Supercomputing . 75 1 197227. arXiv  1702.07908 . doi  10.1007s112270171994x . S2CID 14135321 .  Viebke Andre Pllana Sabri 2015. The Potential of the Intel R Xeon Phi for Supervised Deep Learning . 2015 IEEE 17th International Conference on High Performance Computing and Communications 2015 IEEE 7th International Symposium on Cyberspace Safety and Security and 2015 IEEE 12th International Conference on Embedded Software and Systems . IEEE Xplore . IEEE 2015. pp. 758765. doi  10.1109HPCCCSSICESS.2015.45 . ISBN 9781479989379 . S2CID 15411954 . Archived from the original on 20230306 . Retrieved 20220331 .  Hinton Geoffrey 2012. ImageNet Classification with Deep Convolutional Neural Networks . NIPS12 Proceedings of the 25th International Conference on Neural Information Processing Systems  Volume 1 . 1  10971105. Archived from the original on 20191220 . Retrieved 20210326  via ACM.  a b c d e Azulay Aharon Weiss Yair 2019. Why do deep convolutional networks generalize so poorly to small image transformations . Journal of Machine Learning Research . 20 184 125. ISSN 15337928 . Archived from the original on 20220331 . Retrieved 20220331 .  a b Géron Aurélien 2019. Handson Machine Learning with ScikitLearn Keras and TensorFlow . Sebastopol CA OReilly Media. ISBN 9781492032649 .  pp. 448  CS231n Convolutional Neural Networks for Visual Recognition . cs231n.github.io . Archived from the original on 20191023 . Retrieved 20170425 .  Nirthika Rajendran Manivannan Siyamalan Ramanan Amirthalingam Wang Ruixuan 20220401. Pooling in convolutional neural networks for medical image analysis a survey and an empirical study . Neural Computing and Applications . 34 7 53215347. doi  10.1007s00521022069538 . ISSN 14333058 . PMC 8804673 . PMID 35125669 .  a b Scherer Dominik Müller Andreas C. Behnke Sven 2010. Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition PDF . Artificial Neural Networks ICANN 20th International Conference on . Thessaloniki Greece Springer. pp. 92101. Archived PDF from the original on 20180403 . Retrieved 20161228 .  Graham Benjamin 20141218. Fractional MaxPooling. arXiv  1412.6071  cs.CV .  Springenberg Jost Tobias Dosovitskiy Alexey Brox Thomas Riedmiller Martin 20141221. Striving for Simplicity The All Convolutional Net. arXiv  1412.6806  cs.LG .  Ma Zhanyu Chang Dongliang Xie Jiyang Ding Yifeng Wen Shaoguo Li Xiaoxu Si Zhongwei Guo Jun 2019. FineGrained Vehicle Classification With Channel Max Pooling Modified CNNs. IEEE Transactions on Vehicular Technology . 68 4. Institute of Electrical and Electronics Engineers IEEE 32243233. doi  10.1109tvt.2019.2899972 . ISSN 00189545 . S2CID 86674074 .  Zafar Afia Aamir Muhammad Mohd Nawi Nazri Arshad Ali Riaz Saman Alruban Abdulrahman Dutta Ashit Kumar Almotairi Sultan 20220829. A Comparison of Pooling Methods for Convolutional Neural Networks . Applied Sciences . 12 17 8643. doi  10.3390app12178643 . ISSN 20763417 .  Gholamalinezhad Hossein Khosravi Hossein 20200916 Pooling Methods in Deep Neural Networks a Review  arXiv  2009.07485  Householder Alston S. June 1941. A theory of steadystate activity in nervefiber networks I. Definitions and preliminary lemmas . The Bulletin of Mathematical Biophysics . 3 2 6369. doi  10.1007BF02478220 . ISSN 00074985 .  Romanuke Vadim 2017. Appropriate number and allocation of ReLUs in convolutional neural networks . Research Bulletin of NTUU Kyiv Polytechnic Institute . 1 1 6978. doi  10.2053518100546.2017.1.88156 .  Xavier Glorot Antoine Bordes Yoshua Bengio 2011. Deep sparse rectifier neural networks PDF . AISTATS. Archived from the original PDF on 20161213 . Retrieved 20230410 . Rectifier and softplus activation functions. The second one is a smooth version of the first.  Krizhevsky A. Sutskever I. Hinton G. E. 2012. Imagenet classification with deep convolutional neural networks PDF . Advances in Neural Information Processing Systems . 1  10971105. Archived PDF from the original on 20220331 . Retrieved 20220331 .  Ribeiro Antonio H. Schön Thomas B. 2021. How Convolutional Neural Networks Deal with Aliasing. ICASSP 2021  2021 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP . pp. 27552759. arXiv  2102.07757 . doi  10.1109ICASSP39728.2021.9414627 . ISBN 9781728176055 . S2CID 231925012 .  Myburgh Johannes C. Mouton Coenraad Davel Marelie H. 2020. Tracking Translation Invariance in CNNS . In Gerber Aurona ed.. Artificial Intelligence Research . Communications in Computer and Information Science. Vol. 1342. Cham Springer International Publishing. pp. 282295. arXiv  2104.05997 . doi  10.10079783030661519_18 . ISBN 9783030661519 . S2CID 233219976 . Archived from the original on 20220122 . Retrieved 20210326 .  Richard Zhang 20190425. Making Convolutional Networks ShiftInvariant Again . OCLC 1106340711 .  Jadeberg Simonyan Zisserman Kavukcuoglu Max Karen Andrew Koray 2015. Spatial Transformer Networks PDF . Advances in Neural Information Processing Systems . 28 . Archived PDF from the original on 20210725 . Retrieved 20210326  via NIPS.  cite journal    CS1 maint multiple names authors list  link   E Sabour Sara Frosst Nicholas Hinton Geoffrey 20171026. Dynamic Routing Between Capsules . OCLC 1106278545 .  cite book    CS1 maint multiple names authors list  link   Matiz Sergio Barner Kenneth E. 20190601. Inductive conformal predictor for convolutional neural networks Applications to active learning for image classification . Pattern Recognition . 90  172182. Bibcode  2019PatRe..90..172M . doi  10.1016j.patcog.2019.01.035 . ISSN 00313203 . S2CID 127253432 . Archived from the original on 20210929 . Retrieved 20210929 .  Wieslander Håkan Harrison Philip J. Skogberg Gabriel Jackson Sonya Fridén Markus Karlsson Johan Spjuth Ola Wählby Carolina February 2021. Deep Learning With Conformal Prediction for Hierarchical Analysis of LargeScale WholeSlide Tissue Images . IEEE Journal of Biomedical and Health Informatics . 25 2 371380. doi  10.1109JBHI.2020.2996300 . ISSN 21682208 . PMID 32750907 . S2CID 219885788 .  Srivastava Nitish C. Geoffrey Hinton Alex Krizhevsky Ilya Sutskever Ruslan Salakhutdinov 2014. Dropout A Simple Way to Prevent Neural Networks from overfitting PDF . Journal of Machine Learning Research . 15 1 19291958. Archived PDF from the original on 20160119 . Retrieved 20150103 .  Regularization of Neural Networks using DropConnect  ICML 2013  JMLR WCP . jmlr.org  10581066. 20130213. Archived from the original on 20170812 . Retrieved 20151217 .  Zeiler Matthew D. Fergus Rob 20130115. Stochastic Pooling for Regularization of Deep Convolutional Neural Networks. arXiv  1301.3557  cs.LG .  a b Platt John Steinkraus Dave Simard Patrice Y. August 2003. Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis  Microsoft Research . Microsoft Research . Archived from the original on 20171107 . Retrieved 20151217 .  Hinton Geoffrey E. Srivastava Nitish Krizhevsky Alex Sutskever Ilya Salakhutdinov Ruslan R. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv  1207.0580  cs.NE .  Dropout A Simple Way to Prevent Neural Networks from Overfitting . jmlr.org . Archived from the original on 20160305 . Retrieved 20151217 .  Hinton Geoffrey 1979. Some demonstrations of the effects of structural descriptions in mental imagery. Cognitive Science . 3 3 231250. doi  10.1016s0364021379800087 .  Rock Irvin. The frame of reference. The legacy of Solomon Asch Essays in cognition and social psychology 1990 243268.  J. Hinton Coursera lectures on Neural Networks 2012 Url httpswww.coursera.orglearnneuralnetworks Archived 20161231 at the Wayback Machine  Dave Gershgorn 18 June 2018. The inside story of how AI got good enough to dominate Silicon Valley . Quartz . Archived from the original on 12 December 2019 . Retrieved 5 October 2018 .  Lawrence Steve C. Lee Giles Ah Chung Tsoi Andrew D. Back 1997. Face Recognition A Convolutional Neural Network Approach. IEEE Transactions on Neural Networks . 8 1 98113. CiteSeerX 10.1.1.92.5813 . doi  10.110972.554195 . PMID 18255614 . S2CID 2883848 .  Le Callet Patrick Christian ViardGaudin Dominique Barba 2006. A Convolutional Neural Network Approach for Objective Video Quality Assessment PDF . IEEE Transactions on Neural Networks . 17 5 13161327. doi  10.1109TNN.2006.879766 . PMID 17001990 . S2CID 221185563 . Archived PDF from the original on 24 February 2021 . Retrieved 17 November 2013 .  ImageNet Large Scale Visual Recognition Competition 2014 ILSVRC2014 . Archived from the original on 5 February 2016 . Retrieved 30 January 2016 .  Szegedy Christian Liu Wei Jia Yangqing Sermanet Pierre Reed Scott E. Anguelov Dragomir Erhan Dumitru Vanhoucke Vincent Rabinovich Andrew 2015. Going deeper with convolutions. IEEE Conference on Computer Vision and Pattern Recognition CVPR 2015 Boston MA USA June 712 2015 . IEEE Computer Society. pp. 19. arXiv  1409.4842 . doi  10.1109CVPR.2015.7298594 . ISBN 9781467369640 .  Russakovsky Olga  Deng Jia Su Hao Krause Jonathan Satheesh Sanjeev Ma Sean Huang Zhiheng Karpathy Andrej  Khosla Aditya Bernstein Michael Berg Alexander C. FeiFei Li 2014. Image Net Large Scale Visual Recognition Challenge. arXiv  1409.0575  cs.CV .  The Face Detection Algorithm Set To Revolutionize Image Search . Technology Review . February 16 2015. Archived from the original on 20 September 2020 . Retrieved 27 October 2017 .  Baccouche Moez Mamalet Franck Wolf Christian Garcia Christophe Baskurt Atilla 20111116. Sequential Deep Learning for Human Action Recognition. In Salah Albert Ali Lepri Bruno eds.. Human Behavior Unterstanding . Lecture Notes in Computer Science. Vol. 7065. Springer Berlin Heidelberg. pp. 2939. CiteSeerX 10.1.1.385.4740 . doi  10.10079783642254468_4 . ISBN 9783642254451 .  Ji Shuiwang Xu Wei Yang Ming Yu Kai 20130101. 3D Convolutional Neural Networks for Human Action Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence . 35 1 221231. CiteSeerX 10.1.1.169.4046 . doi  10.1109TPAMI.2012.59 . ISSN 01628828 . PMID 22392705 . S2CID 1923924 .  Huang Jie Zhou Wengang Zhang Qilin Li Houqiang Li Weiping 2018. Videobased Sign Language Recognition without Temporal Segmentation. arXiv  1801.10111  cs.CV .  Karpathy Andrej et al.  Largescale video classification with convolutional neural networks Archived 20190806 at the Wayback Machine . IEEE Conference on Computer Vision and Pattern Recognition CVPR. 2014.  Simonyan Karen Zisserman Andrew 2014. TwoStream Convolutional Networks for Action Recognition in Videos. arXiv  1406.2199  cs.CV . 2014.  Wang Le Duan Xuhuan Zhang Qilin Niu Zhenxing Hua Gang Zheng Nanning 20180522. SegmentTube SpatioTemporal Action Localization in Untrimmed Videos with PerFrame Segmentation PDF . Sensors . 18 5 1657. Bibcode  2018Senso..18.1657W . doi  10.3390s18051657 . ISSN 14248220 . PMC 5982167 . PMID 29789447 . Archived PDF from the original on 20210301 . Retrieved 20180914 .  Duan Xuhuan Wang Le Zhai Changbo Zheng Nanning Zhang Qilin Niu Zhenxing Hua Gang 2018. Joint SpatioTemporal Action Localization in Untrimmed Videos with PerFrame Segmentation. 2018 25th IEEE International Conference on Image Processing ICIP . 25th IEEE International Conference on Image Processing ICIP. pp. 918922. doi  10.1109icip.2018.8451692 . ISBN 9781479970612 .  Taylor Graham W. Fergus Rob LeCun Yann Bregler Christoph 20100101. Convolutional Learning of Spatiotemporal Features . Proceedings of the 11th European Conference on Computer Vision Part VI. ECCV10. Berlin Heidelberg SpringerVerlag. pp. 140153. ISBN 9783642155666 . Archived from the original on 20220331 . Retrieved 20220331 .  Le Q. V. Zou W. Y. Yeung S. Y. Ng A. Y. 20110101. Learning hierarchical invariant spatiotemporal features for action recognition with independent subspace analysis. CVPR 2011 . CVPR 11. Washington DC US IEEE Computer Society. pp. 33613368. CiteSeerX 10.1.1.294.5948 . doi  10.1109CVPR.2011.5995496 . ISBN 9781457703942 . S2CID 6006618 .  Grefenstette Edward Blunsom Phil de Freitas Nando Hermann Karl Moritz 20140429. A Deep Architecture for Semantic Parsing. arXiv  1404.7296  cs.CL .  Mesnil Gregoire Deng Li Gao Jianfeng He Xiaodong Shen Yelong April 2014. Learning Semantic Representations Using Convolutional Neural Networks for Web Search  Microsoft Research . Microsoft Research . Archived from the original on 20170915 . Retrieved 20151217 .  Kalchbrenner Nal Grefenstette Edward Blunsom Phil 20140408. A Convolutional Neural Network for Modelling Sentences. arXiv  1404.2188  cs.CL .  Kim Yoon 20140825. Convolutional Neural Networks for Sentence Classification. arXiv  1408.5882  cs.CL .  Collobert Ronan and Jason Weston.  A unified architecture for natural language processing Deep neural networks with multitask learning Archived 20190904 at the Wayback Machine .Proceedings of the 25th international conference on Machine learning. ACM 2008.  Collobert Ronan Weston Jason Bottou Leon Karlen Michael Kavukcuoglu Koray Kuksa Pavel 20110302. Natural Language Processing almost from Scratch. arXiv  1103.0398  cs.LG .  Yin W Kann K Yu M Schütze H 20170302. Comparative study of CNN and RNN for natural language processing. arXiv  1702.01923  cs.LG .  Bai S. Kolter J.S. Koltun V. 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv  1803.01271  cs.LG .  Gruber N. 2021. Detecting dynamics of action in text with a recurrent neural network. Neural Computing and Applications . 33 12 1570915718. doi  10.1007S00521021061905 . S2CID 236307579 .  Haotian J. Zhong Li Qianxiao Li 2021. Approximation Theory of Convolutional Architectures for Time Series Modelling. International Conference on Machine Learning . arXiv  2107.09355 .  Ren Hansheng Xu Bixiong Wang Yujing Yi Chao Huang Congrui Kou Xiaoyu Xing Tony Yang Mao Tong Jie Zhang Qi 2019. TimeSeries Anomaly Detection Service at Microsoft  Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery  Data Mining . arXiv  1906.03821 . doi  10.11453292500.3330680 . S2CID 182952311 .  Wallach Izhar Dzamba Michael Heifets Abraham 20151009. AtomNet A Deep Convolutional Neural Network for Bioactivity Prediction in Structurebased Drug Discovery. arXiv  1510.02855  cs.LG .  Yosinski Jason Clune Jeff Nguyen Anh Fuchs Thomas Lipson Hod 20150622. Understanding Neural Networks Through Deep Visualization. arXiv  1506.06579  cs.CV .  Toronto startup has a faster way to discover effective medicines . The Globe and Mail . Archived from the original on 20151020 . Retrieved 20151109 .  Startup Harnesses Supercomputers to Seek Cures . KQED Future of You . 20150527. Archived from the original on 20181206 . Retrieved 20151109 .  Chellapilla K Fogel DB 1999. Evolving neural networks to play checkers without relying on expert knowledge. IEEE Trans Neural Netw . 10 6 138291. doi  10.110972.809083 . PMID 18252639 .  Chellapilla K. Fogel D.B. 2001. Evolving an expert checkers playing program without using human expertise. IEEE Transactions on Evolutionary Computation . 5 4 422428. doi  10.11094235.942536 .  Fogel David 2001. Blondie24 Playing at the Edge of AI . San Francisco CA Morgan Kaufmann. ISBN 9781558607835 .  Clark Christopher Storkey Amos 2014. Teaching Deep Convolutional Neural Networks to Play Go. arXiv  1412.3409  cs.AI .  Maddison Chris J. Huang Aja Sutskever Ilya Silver David 2014. Move Evaluation in Go Using Deep Convolutional Neural Networks. arXiv  1412.6564  cs.LG .  AlphaGo  Google DeepMind . Archived from the original on 30 January 2016 . Retrieved 30 January 2016 .  Bai Shaojie Kolter J. Zico Koltun Vladlen 20180419. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. arXiv  1803.01271  cs.LG .  Yu Fisher Koltun Vladlen 20160430. MultiScale Context Aggregation by Dilated Convolutions. arXiv  1511.07122  cs.CV .  Borovykh Anastasia Bohte Sander Oosterlee Cornelis W. 20180917. Conditional Time Series Forecasting with Convolutional Neural Networks. arXiv  1703.04691  stat.ML .  Mittelman Roni 20150803. Timeseries modeling with undecimated fully convolutional neural networks. arXiv  1508.00317  stat.ML .  Chen Yitian Kang Yanfei Chen Yixiong Wang Zizhuo 20190611. Probabilistic Forecasting with Temporal Convolutional Neural Network. arXiv  1906.04397  stat.ML .  Zhao Bendong Lu Huanzhang Chen Shangfeng Liu Junliang Wu Dongya 20170201. Convolutional neural networks for time series classi. Journal of Systems Engineering and Electronics . 28 1 162169. doi  10.21629JSEE.2017.01.18 .  Petneházi Gábor 20190821. QCNN Quantile Convolutional Neural Network. arXiv  1908.07978  cs.LG .  Hubert Mara 20190607 HeiCuBeDa Hilprecht  Heidelberg Cuneiform Benchmark Dataset for the Hilprecht Collection in German heiDATA  institutional repository for research data of Heidelberg University doi  10.11588dataIE8CCN  Hubert Mara and Bartosz Bogacz 2019 Breaking the Code on Broken Tablets The Learning Challenge for Annotated Cuneiform Script in Normalized 2D and 3D Datasets Proceedings of the 15th International Conference on Document Analysis and Recognition ICDAR in German Sydney Australien pp. 148153 doi  10.1109ICDAR.2019.00032  ISBN 9781728130149  S2CID 211026941  Bogacz Bartosz Mara Hubert 2020 Period Classification of 3D Cuneiform Tablets with Geometric Neural Networks Proceedings of the 17th International Conference on Frontiers of Handwriting Recognition ICFHR  Dortmund Germany  Presentation of the ICFHR paper on Period Classification of 3D Cuneiform Tablets with Geometric Neural Networks on YouTube  Durjoy Sen Maitra Ujjwal Bhattacharya S.K. Parui CNN based common approach to handwritten character recognition of multiple scripts Archived 20231016 at the Wayback Machine  in Document Analysis and Recognition ICDAR 2015 13th International Conference on vol. no. pp.10211025 2326 Aug. 2015  NIPS 2017 . Interpretable ML Symposium . 20171020. Archived from the original on 20190907 . Retrieved 20180912 .  Zang Jinliang Wang Le Liu Ziyi Zhang Qilin Hua Gang Zheng Nanning 2018. AttentionBased Temporal Weighted Convolutional Neural Network for Action Recognition. Artificial Intelligence Applications and Innovations . IFIP Advances in Information and Communication Technology. Vol. 519. Cham Springer International Publishing. pp. 97108. arXiv  1803.07179 . doi  10.10079783319920078_9 . ISBN 9783319920061 . ISSN 18684238 . S2CID 4058889 .  Wang Le Zang Jinliang Zhang Qilin Niu Zhenxing Hua Gang Zheng Nanning 20180621. Action Recognition by an AttentionAware Temporal Weighted Convolutional Neural Network PDF . Sensors . 18 7 1979. Bibcode  2018Senso..18.1979W . doi  10.3390s18071979 . ISSN 14248220 . PMC 6069475 . PMID 29933555 . Archived PDF from the original on 20180913 . Retrieved 20180914 .  Ong Hao Yi Chavez Kevin Hong Augustus 20150818. Distributed Deep QLearning. arXiv  1508.04186v2  cs.LG .  Mnih Volodymyr et al. 2015. Humanlevel control through deep reinforcement learning. Nature . 518 7540 529533. Bibcode  2015Natur.518..529M . doi  10.1038nature14236 . PMID 25719670 . S2CID 205242740 .  Sun R. Sessions C. June 2000. Selfsegmentation of sequences automatic formation of hierarchies of sequential behaviors. IEEE Transactions on Systems Man and Cybernetics  Part B Cybernetics . 30 3 403418. CiteSeerX 10.1.1.11.226 . doi  10.11093477.846230 . ISSN 10834419 . PMID 18252373 .  Convolutional Deep Belief Networks on CIFAR10 PDF . Archived PDF from the original on 20170830 . Retrieved 20170818 .  Lee Honglak Grosse Roger Ranganath Rajesh Ng Andrew Y. 1 January 2009. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. Proceedings of the 26th Annual International Conference on Machine Learning . ACM. pp. 609616. CiteSeerX 10.1.1.149.6800 . doi  10.11451553374.1553453 . ISBN 9781605585161 . S2CID 12008458 .  Behnke Sven 2003. Hierarchical Neural Networks for Image Interpretation PDF . Lecture Notes in Computer Science. Vol. 2766. Springer. doi  10.1007b11963 . ISBN 9783540407225 . S2CID 1304548 . Archived PDF from the original on 20170810 . Retrieved 20161228 .  Cade Metz May 18 2016. Google Built Its Very Own Chips to Power Its AI Bots . Wired . Archived from the original on January 13 2018 . Retrieved March 6 2017 . External links  CS231n Convolutional Neural Networks for Visual Recognition  Andrej Karpathy s Stanford computer science course on CNNs in computer vision vdumoulinconv_arithmetic A technical report on convolution arithmetic in the context of deep learning . Animations of convolutions. v t e Differentiable computing General Differentiable programming Information geometry Statistical manifold Automatic differentiation Neuromorphic engineering Pattern recognition Tensor calculus Computational learning theory Inductive bias Concepts Parameter Hyperparameter Loss functions Regression Biasvariance tradeoff Double descent Overfitting Clustering Gradient descent SGD QuasiNewton method Conjugate gradient descent Backpropagation Attention Convolution Normalization Batchnorm Activation Softmax Sigmoid Rectifier Gating Weight initialization Regularization Datasets Augmentation Reinforcement learning Qlearning SARSA Imitation Diffusion Autoregression Adversary Hallucination Applications Machine learning Incontext learning Artificial neural network Deep learning Scientific computing Artificial Intelligence Language model Large language model NMT Hardware IPU TPU VPU Memristor SpiNNaker Software libraries TensorFlow PyTorch Keras scikitlearn Theano JAX Flux.jl MindSpore Implementations Audiovisual AlexNet WaveNet Human image synthesis HWR OCR Speech synthesis Speech recognition Facial recognition AlphaFold Texttoimage models Latent diffusion model DALLE Midjourney Stable Diffusion Texttovideo models Sora VideoPoet Whisper Text Word2vec Seq2seq GloVe BERT T5 Llama Chinchilla AI PaLM GPT 1 J 2 3 ChatGPT 4 Claude Gemini LaMDA Bard BLOOM Project Debater IBM Watson IBM Watsonx Granite PanGuΣ Decisional AlphaGo AlphaZero OpenAI Five Selfdriving car MuZero Action selection AutoGPT Robot control People Frank Rosenblatt Bernard Widrow Paul Werbos Yoshua Bengio Alex Graves Ian Goodfellow Stephen Grossberg Demis Hassabis Geoffrey Hinton Yann LeCun FeiFei Li Andrew Ng Jürgen Schmidhuber David Silver Ilya Sutskever Organizations Anthropic EleutherAI Google DeepMind Hugging Face OpenAI Meta AI Mila MIT CSAIL Huawei Architectures Neural Turing machine Differentiable neural computer Transformer Vision transformer ViT Recurrent neural network RNN Long shortterm memory LSTM Gated recurrent unit GRU Echo state network Multilayer perceptron MLP Convolutional neural network CNN Residual neural network RNN Highway network Mamba Autoencoder Variational autoencoder VAE Generative adversarial network GAN Graph neural network GNN Portals Computer programming Technology Categories Artificial neural networks Machine learning
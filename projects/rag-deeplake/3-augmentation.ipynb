{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain-together==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import openai\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_together import ChatTogether\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = './contents/llm.txt'\n",
    "dataset_path = os.environ['ACTIVELOOP_DATASET']\n",
    "\n",
    "\n",
    "\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(dataset_path=dataset_path, embedding=gemini_embedding, read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    #return db.search(prompt, search_type=\"similarity\")\n",
    "    return db.similarity_search_with_score(query=query, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=\"Tell me about space exploration on the Moon and Mars.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(user_prompt)\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=80):\n",
    "    lines = []\n",
    "    while len(text) > width:\n",
    "        split_index = text.rfind(' ', 0, width)\n",
    "        if split_index == -1:\n",
    "            split_index = width\n",
    "        lines.append(text[:split_index])\n",
    "        text = text[split_index:].strip()\n",
    "    lines.append(text)\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "top_score = 0\n",
    "top_text = \"\"\n",
    "top_metadata = \"\"\n",
    "\n",
    "for document, score in search_results:\n",
    "    print(f\"Document: {document.page_content}, Score: {score}\")\n",
    "\n",
    "    # Assuming the search results are ordered with the top result first\n",
    "    top_score = score\n",
    "    top_text = document.page_content.strip()\n",
    "    top_metadata = document.metadata#['source']\n",
    "\n",
    "    # Print the top search result\n",
    "    print(\"Top Search Result:\")\n",
    "    print(f\"Score: {top_score}\")\n",
    "    print(f\"Source: {top_metadata}\")\n",
    "    print(\"Text:\")\n",
    "    print(wrap_text(top_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input = f\"\"\"{user_prompt} {top_text}\"\"\"\n",
    "print(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_with_augmented_text(context):\n",
    "    text_input = '\\n'.join(context)\n",
    "\n",
    "    prompt = f\"Please summarize or explain the following context: \\n {text_input}\"\n",
    "    try:\n",
    "        # response = openai.chat.completions.create(\n",
    "        #     model='gpt-4o-mini',\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"You are a space exploration expert.\"},\n",
    "        #         {\"role\": \"assistant\", \"content\": \"You can read the input and answer in detail.\"},\n",
    "        #         {\"role\": \"user\", \"content\": prompt}\n",
    "        #     ],\n",
    "        #     temperature=0.1  # Fine-tune parameters as needed\n",
    "        # )\n",
    "        # return response.choices[0].message.content\n",
    "        llm = ChatTogether(model=\"meta-llama/Llama-3-70b-chat-hf\", temperature=0)\n",
    "\n",
    "        messages = [\n",
    "                (\n",
    "                    \"system\", \"You are a space exploration expert\",\n",
    "                ),\n",
    "                (\n",
    "                    \"assistant\",\"You can read the input and answer in detail.\"\n",
    "                ),\n",
    "                (\n",
    "                    \"human\", f'{prompt}'\n",
    "                ),\n",
    "        ]\n",
    "        result = llm.invoke(messages)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Start timing before the request\n",
    "gpt4_response = call_gpt_with_augmented_text(augmented_input)\n",
    "\n",
    "response_time = time.time() - start_time  # Measure response time\n",
    "print(f\"Response Time: {response_time:.2f} seconds\")  # Print response time\n",
    "\n",
    "print('gpt-4o-mini', \"Response:\", gpt4_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Check for markdown by looking for patterns like headers, bold, lists, etc.\n",
    "    markdown_patterns = [\n",
    "        r\"^#+\\s\",           # Headers\n",
    "        r\"^\\*+\",            # Bullet points\n",
    "        r\"\\*\\*\",            # Bold\n",
    "        r\"_\",               # Italics\n",
    "        #r\"\",                # Links\n",
    "        r\"-\\s\",             # Dashes used for lists\n",
    "        r\"\\`\\`\\`\"           # Code blocks\n",
    "    ]\n",
    "\n",
    "    # If any pattern matches, assume the response is in markdown\n",
    "    if any(re.search(pattern, response, re.MULTILINE) for pattern in markdown_patterns):\n",
    "        # Markdown detected, convert to HTML for nicer display\n",
    "        html_output = markdown.markdown(response)\n",
    "        display(HTML(html_output))  # Use display(HTML()) to render HTML in Colab\n",
    "    else:\n",
    "        # No markdown detected, wrap and print as plain text\n",
    "        wrapper = textwrap.TextWrapper(width=80)\n",
    "        wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "        print(\"Text Response:\")\n",
    "        print(\"--------------------\")\n",
    "        print(wrapped_text)\n",
    "        print(\"--------------------\\n\")\n",
    "\n",
    "print_formatted_response(gpt4_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-llm-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
